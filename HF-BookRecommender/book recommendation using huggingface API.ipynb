{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:03:56.120753Z",
     "iopub.status.busy": "2025-03-03T14:03:56.120456Z",
     "iopub.status.idle": "2025-03-03T14:04:17.522304Z",
     "shell.execute_reply": "2025-03-03T14:04:17.521242Z",
     "shell.execute_reply.started": "2025-03-03T14:03:56.120731Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.37 (from langchain-community)\n",
      "  Downloading langchain_core-0.3.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.19 (from langchain-community)\n",
      "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.12)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.19->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (2.11.0a2)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (2.29.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\n",
      "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.40-py3-none-any.whl (414 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: faiss-gpu, python-dotenv, httpx-sse, async-timeout, huggingface_hub, pydantic-settings, langchain-core, langchain-text-splitters, scikit-learn, langchain, sentence-transformers, langchain-community\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.29.0\n",
      "    Uninstalling huggingface-hub-0.29.0:\n",
      "      Successfully uninstalled huggingface-hub-0.29.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.25\n",
      "    Uninstalling langchain-core-0.3.25:\n",
      "      Successfully uninstalled langchain-core-0.3.25\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.3\n",
      "    Uninstalling langchain-text-splitters-0.3.3:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.12\n",
      "    Uninstalling langchain-0.3.12:\n",
      "      Successfully uninstalled langchain-0.3.12\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.3.1\n",
      "    Uninstalling sentence-transformers-3.3.1:\n",
      "      Successfully uninstalled sentence-transformers-3.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed async-timeout-4.0.3 faiss-gpu-1.7.2 httpx-sse-0.4.0 huggingface_hub-0.29.1 langchain-0.3.19 langchain-community-0.3.18 langchain-core-0.3.40 langchain-text-splitters-0.3.6 pydantic-settings-2.8.1 python-dotenv-1.0.1 scikit-learn-1.6.1 sentence-transformers-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-community pandas numpy scikit-learn faiss-gpu sentence-transformers huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:04:17.523751Z",
     "iopub.status.busy": "2025-03-03T14:04:17.523509Z",
     "iopub.status.idle": "2025-03-03T14:04:19.439450Z",
     "shell.execute_reply": "2025-03-03T14:04:19.438762Z",
     "shell.execute_reply.started": "2025-03-03T14:04:17.523718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:04:19.441435Z",
     "iopub.status.busy": "2025-03-03T14:04:19.441146Z",
     "iopub.status.idle": "2025-03-03T14:04:19.444887Z",
     "shell.execute_reply": "2025-03-03T14:04:19.444066Z",
     "shell.execute_reply.started": "2025-03-03T14:04:19.441407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the Hugging Face API token\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Replace with your own API KEY\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:04:19.446223Z",
     "iopub.status.busy": "2025-03-03T14:04:19.445949Z",
     "iopub.status.idle": "2025-03-03T14:04:30.616065Z",
     "shell.execute_reply": "2025-03-03T14:04:30.615434Z",
     "shell.execute_reply.started": "2025-03-03T14:04:19.446191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/kaggle/input/bookcrossing-dataset/Books Data with Category Language and Summary/Preprocessed_data.csv\"\n",
    "\n",
    "try:\n",
    "    prepro_data = pd.read_csv(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset not found at\", DATA_PATH)\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:04:30.617330Z",
     "iopub.status.busy": "2025-03-03T14:04:30.616992Z",
     "iopub.status.idle": "2025-03-03T14:04:32.602834Z",
     "shell.execute_reply": "2025-03-03T14:04:32.602187Z",
     "shell.execute_reply.started": "2025-03-03T14:04:30.617299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Filtering and preparing the dataset\"\"\"\n",
    "\n",
    "new_book = prepro_data.drop(columns=['Unnamed: 0', 'location', 'age', 'year_of_publication', 'publisher', 'img_s', 'img_l',\n",
    "                                     'img_m', 'Language', 'city', 'state', 'country','user_id','isbn','rating'])\n",
    "\n",
    "new_book['book_author'] = new_book['book_author'].fillna('Unknown')\n",
    "\n",
    "# let's apply regex to make thing simple\n",
    "#let's clean the book title\n",
    "def clean_book_title(title):\n",
    "    return re.sub(\"[^a-zA-Z0-9 ]\", \"\",title)\n",
    "new_book['clean_book_title'] = new_book['book_title'].apply(clean_book_title)\n",
    "\n",
    "new_unique_book = new_book.drop_duplicates(subset=['clean_book_title','book_author','Summary'])\n",
    "\n",
    "new_unique_book = new_unique_book[['clean_book_title', 'Category','Summary', 'book_author']]\n",
    "\n",
    "new_unique_book.columns = ['book_title', 'Category','Summary', 'book_author'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:04:32.603757Z",
     "iopub.status.busy": "2025-03-03T14:04:32.603554Z",
     "iopub.status.idle": "2025-03-03T14:04:32.623958Z",
     "shell.execute_reply": "2025-03-03T14:04:32.623212Z",
     "shell.execute_reply.started": "2025-03-03T14:04:32.603740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>Category</th>\n",
       "      <th>Summary</th>\n",
       "      <th>book_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>['Social Science']</td>\n",
       "      <td>Provides an introduction to classical myths pl...</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            book_title            Category  \\\n",
       "0  Classical Mythology  ['Social Science']   \n",
       "1         Clara Callan       ['Actresses']   \n",
       "\n",
       "                                             Summary           book_author  \n",
       "0  Provides an introduction to classical myths pl...    Mark P. O. Morford  \n",
       "1  In a small town in Canada, Clara Callan reluct...  Richard Bruce Wright  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_unique_book.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:04:32.625122Z",
     "iopub.status.busy": "2025-03-03T14:04:32.624800Z",
     "iopub.status.idle": "2025-03-03T14:04:34.662016Z",
     "shell.execute_reply": "2025-03-03T14:04:34.661329Z",
     "shell.execute_reply.started": "2025-03-03T14:04:32.625065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combines the title and the summary together.\n",
    "def combine_title_summary(row):\n",
    "    return f\"{row['book_title']} {row['Summary']}\" \n",
    "\n",
    "new_unique_book['combined_text'] = new_unique_book.apply(combine_title_summary, axis=1)\n",
    "\n",
    "\n",
    "SAMPLE_SIZE = 1000  \n",
    "df_sampled = new_unique_book.sample(n=SAMPLE_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:04:34.664186Z",
     "iopub.status.busy": "2025-03-03T14:04:34.663928Z",
     "iopub.status.idle": "2025-03-03T14:04:34.672230Z",
     "shell.execute_reply": "2025-03-03T14:04:34.671446Z",
     "shell.execute_reply.started": "2025-03-03T14:04:34.664167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>Category</th>\n",
       "      <th>Summary</th>\n",
       "      <th>book_author</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>849808</th>\n",
       "      <td>Young Indiana Jones and the Princess of Peril</td>\n",
       "      <td>['Adventure and adventurers']</td>\n",
       "      <td>In 1913 Russia, Indy befriends a young Georgia...</td>\n",
       "      <td>Les Martin</td>\n",
       "      <td>Young Indiana Jones and the Princess of Peril ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884496</th>\n",
       "      <td>Once upon a Time</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>Prince Ruyen of the Isle of Falcon Bruine has ...</td>\n",
       "      <td>Constance O'Banyon</td>\n",
       "      <td>Once upon a Time Prince Ruyen of the Isle of F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           book_title  \\\n",
       "849808  Young Indiana Jones and the Princess of Peril   \n",
       "884496                               Once upon a Time   \n",
       "\n",
       "                             Category  \\\n",
       "849808  ['Adventure and adventurers']   \n",
       "884496                    ['Fiction']   \n",
       "\n",
       "                                                  Summary         book_author  \\\n",
       "849808  In 1913 Russia, Indy befriends a young Georgia...          Les Martin   \n",
       "884496  Prince Ruyen of the Isle of Falcon Bruine has ...  Constance O'Banyon   \n",
       "\n",
       "                                            combined_text  \n",
       "849808  Young Indiana Jones and the Princess of Peril ...  \n",
       "884496  Once upon a Time Prince Ruyen of the Isle of F...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:04:34.673750Z",
     "iopub.status.busy": "2025-03-03T14:04:34.673439Z",
     "iopub.status.idle": "2025-03-03T14:05:10.085675Z",
     "shell.execute_reply": "2025-03-03T14:05:10.084672Z",
     "shell.execute_reply.started": "2025-03-03T14:04:34.673720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Book Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-95501513d37f>:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d97d3363cd46388483a48e3f773967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e4a8cbfa064d36988586c366d3cb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805d217ceb1d48e59d9c2e74506a51c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87fa27049c84d2d941c47f82e1ae4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d69334a46540dca823a5ac38b43343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e71396de2e4aeda97c103c01a0c76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5401addfc534038b42261fae7680090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b68d7f6924f4e9bbd47b1cbbaba83a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28811ca0fb7343a1b83e55d93b026aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30ca46d642944eb8214dcd6f054446e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e13d49b5f654b009921693de2efe886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"This Embedding is not giving us good result\"\"\"\n",
    "\n",
    "# Create Book Embeddings\n",
    "print(\"Creating Book Embeddings...\")\n",
    "model_name = \"all-mpnet-base-v2\" \n",
    "\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:05:10.087156Z",
     "iopub.status.busy": "2025-03-03T14:05:10.086537Z",
     "iopub.status.idle": "2025-03-03T14:05:13.890162Z",
     "shell.execute_reply": "2025-03-03T14:05:13.889273Z",
     "shell.execute_reply.started": "2025-03-03T14:05:10.087125Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing Embeddings in FAISS...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Documents for FAISS\n",
    "loader = DataFrameLoader(df_sampled, page_content_column=\"combined_text\")\n",
    "documents = loader.load()\n",
    "\n",
    "#Store Embeddings in FAISS\n",
    "print(\"Storing Embeddings in FAISS...\")\n",
    "db = FAISS.from_documents(documents, hf_embeddings)\n",
    "\n",
    "#clean up memory\n",
    "del prepro_data, new_book, new_unique_book, df_sampled, loader, documents\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:05:14.045215Z",
     "iopub.status.busy": "2025-03-03T14:05:14.044981Z",
     "iopub.status.idle": "2025-03-03T14:05:14.063781Z",
     "shell.execute_reply": "2025-03-03T14:05:14.063166Z",
     "shell.execute_reply.started": "2025-03-03T14:05:14.045196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def recommend_books(user_input, num_recommendations=3):\n",
    "    \"\"\"\n",
    "    Recommends books based on user input using embeddings for similarity search\n",
    "    and an LLM (via Hugging Face Inference API) to generate recommendations.\n",
    "    \"\"\"\n",
    "    print(\"Generating Recommendations...\")\n",
    "\n",
    "    # Find similar books using FAISS embeddings\n",
    "    similar_books = db.similarity_search(user_input, k=5)\n",
    "    book_titles = \"\\n\".join([f\"Title: {doc.page_content}\" for doc in similar_books])\n",
    "    \n",
    "    print(\"--- FAISS Similar Books ---\")\n",
    "    print(book_titles)\n",
    "    print(\"--- End FAISS Similar Books ---\")\n",
    "    \n",
    "    print(\"Accessing HF LLM\")   \n",
    "    \n",
    "    \n",
    "    repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # \"google/flan-t5-small\" \n",
    "    llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    temperature=0.6,\n",
    "    max_new_tokens=150, \n",
    "    wait_for_model=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are a book recommendation expert. The user likes: {user_input}\n",
    "\n",
    "    Here are some books similar to what the user likes:\n",
    "    {book_titles}\n",
    "\n",
    "    Based on this information, recommend THREE books. For EACH book, provide the title and a VERY BRIEF (one-sentence) explanation of why the user might like it.\n",
    "    Be concise and engaging. Do NOT include website URLs or marketing content. Do NOT recommend books already listed.\n",
    "    For each book, provide the title and the summary.\n",
    "\n",
    "    Example Recommendations:\n",
    "    - \"The Name of the Wind (Summary: A beautifully written fantasy novel with a compelling protagonist.): A fantastical adventure\"\n",
    "    - \"Pride and Prejudice (Summary: A classic romance with witty dialogue and memorable characters.): A romance story\"\n",
    "    - \"The Martian (Summary: A thrilling science fiction story about survival against all odds.): Sci-fi.\"\n",
    "\n",
    "    Recommendations:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=prompt_template,\n",
    "                              input_variables=[\"user_input\", \"book_titles\"])\n",
    "    \n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    \n",
    "    # Retry mechanism in case of transient 503 errors\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            recommendations = llm_chain.run({\"user_input\": user_input, \"book_titles\": book_titles})\n",
    "            break \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            time.sleep(2)  \n",
    "            if attempt == max_retries - 1:\n",
    "                return \"Sorry, I encountered an error generating recommendations. Please try again later.\"\n",
    "    \n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T14:05:14.064846Z",
     "iopub.status.busy": "2025-03-03T14:05:14.064586Z",
     "iopub.status.idle": "2025-03-03T14:06:33.088429Z",
     "shell.execute_reply": "2025-03-03T14:06:33.087527Z",
     "shell.execute_reply.started": "2025-03-03T14:05:14.064817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a book you like, a genre, or a topic you're interested in:  time travel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running. Please wait...\n",
      "Generating Recommendations...\n",
      "--- FAISS Similar Books ---\n",
      "Title: You See the Future Choose Your Own Adventure No 44 The reader&#39;s decisions will determine what happens when a bump on\n",
      "the head leaves the reader with the ability to see the future.\n",
      "Title: A question of time A Red badge novel of suspense 9\n",
      "Title: The Theory of Relativity  Other Essays 9\n",
      "Title: Serendipity 9\n",
      "Title: Tarot 9\n",
      "--- End FAISS Similar Books ---\n",
      "Accessing HF LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-7289cd463f97>:24: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceEndpoint(\n",
      "<ipython-input-14-7289cd463f97>:54: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
      "<ipython-input-14-7289cd463f97>:60: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  recommendations = llm_chain.run({\"user_input\": user_input, \"book_titles\": book_titles})\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations:\n",
      "1. \"The Name of the Wind\" by Patrick Rothfuss\n",
      "    2. \"Pride and Prejudice\" by Jane Austen\n",
      "    3. \"The Martian\" by Andy Weir\n",
      "\n",
      "I hope that helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "#Get User Input and Print Recommendations\n",
    "user_input = input(\"Enter a book you like, a genre, or a topic you're interested in: \")\n",
    "print(\"Running. Please wait...\")\n",
    "\n",
    "recommendations = recommend_books(user_input)\n",
    "print(\"\\nRecommendations:\")\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 825851,
     "sourceId": 1950576,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
